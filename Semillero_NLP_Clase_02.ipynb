{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsmaelArista/Global-Hitss/blob/main/Semillero_NLP_Clase_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌐 Introducción al Procesamiento de Lenguaje Natural (NLP)\n",
        "\n",
        "En esta clase aprenderemos los conceptos básicos del **Procesamiento de Lenguaje Natural (NLP)**, un área de la inteligencia artificial enfocada en permitir que las computadoras comprendan y procesen el lenguaje humano.\n",
        "\n",
        "**Temas a tratar:**\n",
        "- Tokenización  \n",
        "- Stopwords  \n",
        "- Lematización  \n",
        "- Stemming  \n",
        "- Representación vectorial (Bag of Words y TF-IDF)\n"
      ],
      "metadata": {
        "id": "PbIxs1TUdtUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📘 Tokenización\n",
        "La **tokenización** es el proceso de dividir un texto en unidades más pequeñas llamadas *tokens*.  \n",
        "Los tokens pueden ser palabras, frases o incluso caracteres.\n",
        "\n",
        "**Ejemplo:**\n",
        "> Texto: \"El NLP es fascinante.\" → Tokens: [\"El\", \"NLP\", \"es\", \"fascinante\"]\n"
      ],
      "metadata": {
        "id": "Z-pLQWFieEu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VprmtwGXdoZh",
        "outputId": "0590feb7-7ac4-49fd-a83e-e8b694f72df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'nlp', 'es', 'una', 'rama', 'fascinante', 'de', 'la', 'inteligencia', 'artificial', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descarga los recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "texto = \"El NLP es una rama fascinante de la inteligencia artificial.\"\n",
        "tokens = word_tokenize(texto.lower())\n",
        "print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚫 Stopwords\n",
        "Las **stopwords** son palabras comunes (como artículos, preposiciones y pronombres) que no aportan mucho significado semántico y suelen eliminarse antes de procesar el texto.\n"
      ],
      "metadata": {
        "id": "ATSDla65ehZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSVdAcWxeJrR",
        "outputId": "69203964-5904-4da3-8feb-7d7ecb4fc9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nlp', 'rama', 'fascinante', 'inteligencia', 'artificial', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords_sp = set(stopwords.words('spanish'))\n",
        "\n",
        "print(\"sí\" in stopwords_sp)\n",
        "print(\"no\" in stopwords_sp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stYkB0mu6rdw",
        "outputId": "6dca6bcf-c33d-4b5d-a9a9-3078ec32a139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.remove(\"no\")  # conservar \"no\"\n",
        "\n"
      ],
      "metadata": {
        "id": "otVJAf_h604A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"sí\" in stopwords_custom)\n",
        "print(\"no\" in stopwords_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArhN32CJ67Zk",
        "outputId": "39540c99-c9a9-4f67-b02f-19bfc9c4626f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.update([\"sí\", \"vale\", \"ok\", \"aja\"])  # agregas las que no aporten significado"
      ],
      "metadata": {
        "id": "M509iXcf7Ts4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌱 Lematización\n",
        "La **lematización** consiste en reducir una palabra a su forma base o *lema*, considerando el contexto y la morfología.\n",
        "\n",
        "**Ejemplo:**  \n",
        "> corriendo → correr\n"
      ],
      "metadata": {
        "id": "G1Kytd0NfOkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar el modelo de idioma español de SpaCy\n",
        "!python -m spacy download es_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44Fgoi8_fkIV",
        "outputId": "71d7371c-7f0e-41c3-abe9-ab5936f43811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')  # Modelo para español\n",
        "\n",
        "texto = \"Los gatos estaban corriendo rápidamente por el jardín.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW8mMplSeqyj",
        "outputId": "fc7d175d-576d-4f62-c737-a19a362eeb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'gato', 'estar', 'correr', 'rápidamente', 'por', 'el', 'jardín', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔪 Stemming\n",
        "El **stemming** recorta las palabras a su raíz, sin considerar reglas lingüísticas.  \n",
        "Es una forma más agresiva que la lematización.\n",
        "\n",
        "**Ejemplo:**  \n",
        "> jugando, jugador, jugó → jug\n"
      ],
      "metadata": {
        "id": "_t473jRmgFFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stems = [stemmer.stem(palabra) for palabra in tokens_filtrados]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YnQOCZkfU-K",
        "outputId": "5ff3e080-9867-48a3-a475-0f28e53480cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nlp', 'ram', 'fascin', 'inteligent', 'artificial', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧮 Representación Vectorial del Texto\n",
        "Una vez que el texto está limpio, debemos representarlo numéricamente para usarlo en modelos de Machine Learning.  \n",
        "Las dos técnicas más comunes son:\n",
        "\n",
        "1. **Bag of Words (BoW)**: cuenta las apariciones de cada palabra.  \n",
        "2. **TF-IDF (Term Frequency - Inverse Document Frequency)**: pondera las palabras según su frecuencia en el corpus.\n"
      ],
      "metadata": {
        "id": "o3TZs_NHmxF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"El NLP es una rama de la inteligencia artificial.\",\n",
        "    \"La tokenización y la lematización son procesos importantes en NLP.\",\n",
        "    \"TF-IDF permite representar textos de forma numérica.\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "print(\"Bag of Words:\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XoWhTlNgSp4",
        "outputId": "15fb9301-daf6-4e92-c3fa-6f4a0078b0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "['artificial' 'de' 'el' 'en' 'es' 'forma' 'idf' 'importantes'\n",
            " 'inteligencia' 'la' 'lematización' 'nlp' 'numérica' 'permite' 'procesos'\n",
            " 'rama' 'representar' 'son' 'textos' 'tf' 'tokenización' 'una']\n",
            "[[1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1 0 2 1 1 0 0 1 0 0 1 0 0 1 0]\n",
            " [0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0]]\n",
            "\n",
            "TF-IDF:\n",
            "['artificial' 'de' 'el' 'en' 'es' 'forma' 'idf' 'importantes'\n",
            " 'inteligencia' 'la' 'lematización' 'nlp' 'numérica' 'permite' 'procesos'\n",
            " 'rama' 'representar' 'son' 'textos' 'tf' 'tokenización' 'una']\n",
            "[[0.35955412 0.27345018 0.35955412 0.         0.35955412 0.\n",
            "  0.         0.         0.35955412 0.27345018 0.         0.27345018\n",
            "  0.         0.         0.         0.35955412 0.         0.\n",
            "  0.         0.         0.         0.35955412]\n",
            " [0.         0.         0.         0.33535157 0.         0.\n",
            "  0.         0.33535157 0.         0.51008702 0.33535157 0.25504351\n",
            "  0.         0.         0.33535157 0.         0.         0.33535157\n",
            "  0.         0.         0.33535157 0.        ]\n",
            " [0.         0.27626457 0.         0.         0.         0.36325471\n",
            "  0.36325471 0.         0.         0.         0.         0.\n",
            "  0.36325471 0.36325471 0.         0.         0.36325471 0.\n",
            "  0.36325471 0.36325471 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Conclusiones\n",
        "\n",
        "En esta clase aprendimos los pasos fundamentales del preprocesamiento de texto:\n",
        "- Tokenización para dividir texto en unidades.  \n",
        "- Eliminación de stopwords.  \n",
        "- Lematización y stemming para normalizar palabras.  \n",
        "- Representación vectorial del texto para modelos de Machine Learning.\n",
        "\n",
        "Estos conceptos son la base de cualquier aplicación moderna de NLP, como chatbots, análisis de sentimiento o motores de búsqueda.\n"
      ],
      "metadata": {
        "id": "GMnOVwijm6jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Mostrar algunas filas\n",
        "print(data.head())\n",
        "\n",
        "corpus = data['tweet'].astype(str).tolist()\n",
        "print(\"Tweets en el corpus:\", len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPH8pfMim5qs",
        "outputId": "07055d4a-9760-4d8f-daa2-fb972c525a2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  label                                              tweet\n",
            "0   1      0   @user when a father is dysfunctional and is s...\n",
            "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
            "2   3      0                                bihday your majesty\n",
            "3   4      0  #model   i love u take with u all the time in ...\n",
            "4   5      0             factsguide: society now    #motivation\n",
            "Tweets en el corpus: 31962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descarga de recursos\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "texto = corpus[0]\n",
        "tokens = word_tokenize(texto.lower())\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW77xr5jC4sr",
        "outputId": "20bf2562-4be4-4164-a936-9ec10bc375a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWijzBA6FC3e",
        "outputId": "02306a43-b863-4d90-f88b-ad807105aad3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords añadidas\n",
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.update([\"sí\", \"vale\", \"ok\", \"aja\"])\n"
      ],
      "metadata": {
        "id": "S6_173nYFqdR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nc_JBTB5N8A",
        "outputId": "612d6383-67f5-48e8-b36e-3457ae5aa5aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematización\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp(corpus[0])\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2-6jCS_Ft1z",
        "outputId": "7347bdce-c9bf-4862-b5cd-18c7ba5e1725"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "[' ', '@user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'haber', 'drags', 'his ', 'kids', 'into', 'his', 'dysfunction', '.', '  ', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stems = [stemmer.stem(palabra) for palabra in tokens_filtrados]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBESO4iB5fsq",
        "outputId": "5c2d1ad0-e8c4-4074-f298-e9e56dfb3d30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'fath', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "print(\"Bag of Words (docs x términos):\", X_bow.shape)\n",
        "print(vectorizer_bow.get_feature_names_out()[:20])  # primeras 20 palabras\n",
        "# print(X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxcLeluf5lI2",
        "outputId": "8e3d99e9-85f0-4d5b-d99b-164205e6437d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (docs x términos): (31962, 41392)\n",
            "['00' '000' '000001' '001' '0099' '00am' '00h30' '00pm' '01' '0115' '0161'\n",
            " '01926889917' '02' '0266808099' '03' '030916' '03111880779' '033' '0345'\n",
            " '039']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
        "print(\"TF-IDF (docs x términos):\", X_tfidf.shape)\n",
        "print(vectorizer_tfidf.get_feature_names_out()[:20])\n",
        "print(X_tfidf.toarray())  # cuidado: puede ser grande\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ql1S6j5q5W",
        "outputId": "bdfeb341-0732-498f-cbe5-3ac76ee224d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF (docs x términos): (31962, 41392)\n",
            "['00' '000' '000001' '001' '0099' '00am' '00h30' '00pm' '01' '0115' '0161'\n",
            " '01926889917' '02' '0266808099' '03' '030916' '03111880779' '033' '0345'\n",
            " '039']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}