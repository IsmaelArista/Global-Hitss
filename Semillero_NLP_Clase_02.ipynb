{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsmaelArista/Global-Hitss/blob/main/Semillero_NLP_Clase_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåê Introducci√≥n al Procesamiento de Lenguaje Natural (NLP)\n",
        "\n",
        "En esta clase aprenderemos los conceptos b√°sicos del **Procesamiento de Lenguaje Natural (NLP)**, un √°rea de la inteligencia artificial enfocada en permitir que las computadoras comprendan y procesen el lenguaje humano.\n",
        "\n",
        "**Temas a tratar:**\n",
        "- Tokenizaci√≥n  \n",
        "- Stopwords  \n",
        "- Lematizaci√≥n  \n",
        "- Stemming  \n",
        "- Representaci√≥n vectorial (Bag of Words y TF-IDF)\n"
      ],
      "metadata": {
        "id": "PbIxs1TUdtUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìò Tokenizaci√≥n\n",
        "La **tokenizaci√≥n** es el proceso de dividir un texto en unidades m√°s peque√±as llamadas *tokens*.  \n",
        "Los tokens pueden ser palabras, frases o incluso caracteres.\n",
        "\n",
        "**Ejemplo:**\n",
        "> Texto: \"El NLP es fascinante.\" ‚Üí Tokens: [\"El\", \"NLP\", \"es\", \"fascinante\"]\n"
      ],
      "metadata": {
        "id": "Z-pLQWFieEu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VprmtwGXdoZh",
        "outputId": "0590feb7-7ac4-49fd-a83e-e8b694f72df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'nlp', 'es', 'una', 'rama', 'fascinante', 'de', 'la', 'inteligencia', 'artificial', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descarga los recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "texto = \"El NLP es una rama fascinante de la inteligencia artificial.\"\n",
        "tokens = word_tokenize(texto.lower())\n",
        "print(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö´ Stopwords\n",
        "Las **stopwords** son palabras comunes (como art√≠culos, preposiciones y pronombres) que no aportan mucho significado sem√°ntico y suelen eliminarse antes de procesar el texto.\n"
      ],
      "metadata": {
        "id": "ATSDla65ehZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSVdAcWxeJrR",
        "outputId": "69203964-5904-4da3-8feb-7d7ecb4fc9bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nlp', 'rama', 'fascinante', 'inteligencia', 'artificial', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords_sp = set(stopwords.words('spanish'))\n",
        "\n",
        "print(\"s√≠\" in stopwords_sp)\n",
        "print(\"no\" in stopwords_sp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stYkB0mu6rdw",
        "outputId": "6dca6bcf-c33d-4b5d-a9a9-3078ec32a139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.remove(\"no\")  # conservar \"no\"\n",
        "\n"
      ],
      "metadata": {
        "id": "otVJAf_h604A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"s√≠\" in stopwords_custom)\n",
        "print(\"no\" in stopwords_custom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArhN32CJ67Zk",
        "outputId": "39540c99-c9a9-4f67-b02f-19bfc9c4626f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.update([\"s√≠\", \"vale\", \"ok\", \"aja\"])  # agregas las que no aporten significado"
      ],
      "metadata": {
        "id": "M509iXcf7Ts4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå± Lematizaci√≥n\n",
        "La **lematizaci√≥n** consiste en reducir una palabra a su forma base o *lema*, considerando el contexto y la morfolog√≠a.\n",
        "\n",
        "**Ejemplo:**  \n",
        "> corriendo ‚Üí correr\n"
      ],
      "metadata": {
        "id": "G1Kytd0NfOkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar el modelo de idioma espa√±ol de SpaCy\n",
        "!python -m spacy download es_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44Fgoi8_fkIV",
        "outputId": "71d7371c-7f0e-41c3-abe9-ab5936f43811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')  # Modelo para espa√±ol\n",
        "\n",
        "texto = \"Los gatos estaban corriendo r√°pidamente por el jard√≠n.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW8mMplSeqyj",
        "outputId": "fc7d175d-576d-4f62-c737-a19a362eeb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'gato', 'estar', 'correr', 'r√°pidamente', 'por', 'el', 'jard√≠n', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî™ Stemming\n",
        "El **stemming** recorta las palabras a su ra√≠z, sin considerar reglas ling√º√≠sticas.  \n",
        "Es una forma m√°s agresiva que la lematizaci√≥n.\n",
        "\n",
        "**Ejemplo:**  \n",
        "> jugando, jugador, jug√≥ ‚Üí jug\n"
      ],
      "metadata": {
        "id": "_t473jRmgFFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stems = [stemmer.stem(palabra) for palabra in tokens_filtrados]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YnQOCZkfU-K",
        "outputId": "5ff3e080-9867-48a3-a475-0f28e53480cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['nlp', 'ram', 'fascin', 'inteligent', 'artificial', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Representaci√≥n Vectorial del Texto\n",
        "Una vez que el texto est√° limpio, debemos representarlo num√©ricamente para usarlo en modelos de Machine Learning.  \n",
        "Las dos t√©cnicas m√°s comunes son:\n",
        "\n",
        "1. **Bag of Words (BoW)**: cuenta las apariciones de cada palabra.  \n",
        "2. **TF-IDF (Term Frequency - Inverse Document Frequency)**: pondera las palabras seg√∫n su frecuencia en el corpus.\n"
      ],
      "metadata": {
        "id": "o3TZs_NHmxF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"El NLP es una rama de la inteligencia artificial.\",\n",
        "    \"La tokenizaci√≥n y la lematizaci√≥n son procesos importantes en NLP.\",\n",
        "    \"TF-IDF permite representar textos de forma num√©rica.\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "print(\"Bag of Words:\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XoWhTlNgSp4",
        "outputId": "15fb9301-daf6-4e92-c3fa-6f4a0078b0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words:\n",
            "['artificial' 'de' 'el' 'en' 'es' 'forma' 'idf' 'importantes'\n",
            " 'inteligencia' 'la' 'lematizaci√≥n' 'nlp' 'num√©rica' 'permite' 'procesos'\n",
            " 'rama' 'representar' 'son' 'textos' 'tf' 'tokenizaci√≥n' 'una']\n",
            "[[1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 0 1 0 0 0 1 0 2 1 1 0 0 1 0 0 1 0 0 1 0]\n",
            " [0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0]]\n",
            "\n",
            "TF-IDF:\n",
            "['artificial' 'de' 'el' 'en' 'es' 'forma' 'idf' 'importantes'\n",
            " 'inteligencia' 'la' 'lematizaci√≥n' 'nlp' 'num√©rica' 'permite' 'procesos'\n",
            " 'rama' 'representar' 'son' 'textos' 'tf' 'tokenizaci√≥n' 'una']\n",
            "[[0.35955412 0.27345018 0.35955412 0.         0.35955412 0.\n",
            "  0.         0.         0.35955412 0.27345018 0.         0.27345018\n",
            "  0.         0.         0.         0.35955412 0.         0.\n",
            "  0.         0.         0.         0.35955412]\n",
            " [0.         0.         0.         0.33535157 0.         0.\n",
            "  0.         0.33535157 0.         0.51008702 0.33535157 0.25504351\n",
            "  0.         0.         0.33535157 0.         0.         0.33535157\n",
            "  0.         0.         0.33535157 0.        ]\n",
            " [0.         0.27626457 0.         0.         0.         0.36325471\n",
            "  0.36325471 0.         0.         0.         0.         0.\n",
            "  0.36325471 0.36325471 0.         0.         0.36325471 0.\n",
            "  0.36325471 0.36325471 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Conclusiones\n",
        "\n",
        "En esta clase aprendimos los pasos fundamentales del preprocesamiento de texto:\n",
        "- Tokenizaci√≥n para dividir texto en unidades.  \n",
        "- Eliminaci√≥n de stopwords.  \n",
        "- Lematizaci√≥n y stemming para normalizar palabras.  \n",
        "- Representaci√≥n vectorial del texto para modelos de Machine Learning.\n",
        "\n",
        "Estos conceptos son la base de cualquier aplicaci√≥n moderna de NLP, como chatbots, an√°lisis de sentimiento o motores de b√∫squeda.\n"
      ],
      "metadata": {
        "id": "GMnOVwijm6jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Mostrar algunas filas\n",
        "print(data.head())\n",
        "\n",
        "corpus = data['tweet'].astype(str).tolist()\n",
        "print(\"Tweets en el corpus:\", len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPH8pfMim5qs",
        "outputId": "07055d4a-9760-4d8f-daa2-fb972c525a2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  label                                              tweet\n",
            "0   1      0   @user when a father is dysfunctional and is s...\n",
            "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
            "2   3      0                                bihday your majesty\n",
            "3   4      0  #model   i love u take with u all the time in ...\n",
            "4   5      0             factsguide: society now    #motivation\n",
            "Tweets en el corpus: 31962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Descarga de recursos\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "texto = corpus[0]\n",
        "tokens = word_tokenize(texto.lower())\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW77xr5jC4sr",
        "outputId": "20bf2562-4be4-4164-a936-9ec10bc375a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWijzBA6FC3e",
        "outputId": "02306a43-b863-4d90-f88b-ad807105aad3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords a√±adidas\n",
        "stopwords_custom = set(stopwords.words('spanish'))\n",
        "stopwords_custom.update([\"s√≠\", \"vale\", \"ok\", \"aja\"])\n"
      ],
      "metadata": {
        "id": "S6_173nYFqdR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Nc_JBTB5N8A",
        "outputId": "612d6383-67f5-48e8-b36e-3457ae5aa5aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematizaci√≥n\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp(corpus[0])\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2-6jCS_Ft1z",
        "outputId": "7347bdce-c9bf-4862-b5cd-18c7ba5e1725"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "[' ', '@user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'haber', 'drags', 'his ', 'kids', 'into', 'his', 'dysfunction', '.', '  ', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('spanish')\n",
        "stems = [stemmer.stem(palabra) for palabra in tokens_filtrados]\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBESO4iB5fsq",
        "outputId": "5c2d1ad0-e8c4-4074-f298-e9e56dfb3d30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'user', 'when', 'fath', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', '.', '#', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of Words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "print(\"Bag of Words (docs x t√©rminos):\", X_bow.shape)\n",
        "print(vectorizer_bow.get_feature_names_out()[:20])  # primeras 20 palabras\n",
        "# print(X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxcLeluf5lI2",
        "outputId": "8e3d99e9-85f0-4d5b-d99b-164205e6437d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (docs x t√©rminos): (31962, 41392)\n",
            "['00' '000' '000001' '001' '0099' '00am' '00h30' '00pm' '01' '0115' '0161'\n",
            " '01926889917' '02' '0266808099' '03' '030916' '03111880779' '033' '0345'\n",
            " '039']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
        "print(\"TF-IDF (docs x t√©rminos):\", X_tfidf.shape)\n",
        "print(vectorizer_tfidf.get_feature_names_out()[:20])\n",
        "print(X_tfidf.toarray())  # cuidado: puede ser grande\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ql1S6j5q5W",
        "outputId": "bdfeb341-0732-498f-cbe5-3ac76ee224d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF (docs x t√©rminos): (31962, 41392)\n",
            "['00' '000' '000001' '001' '0099' '00am' '00h30' '00pm' '01' '0115' '0161'\n",
            " '01926889917' '02' '0266808099' '03' '030916' '03111880779' '033' '0345'\n",
            " '039']\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}